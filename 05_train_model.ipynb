{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698fc22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Dataset Class ---\n",
    "class RecSysDataset(Dataset):\n",
    "    def __init__(self, parquet_file):\n",
    "        self.data = pd.read_parquet(parquet_file)\n",
    "        \n",
    "        # Features NumÃ©ricas normalizadas (Gambi de engenheiro: dividir pelo max)\n",
    "        self.users = torch.LongTensor(self.data['user_index'].values)\n",
    "        self.items = torch.LongTensor(self.data['item_index'].values)\n",
    "        \n",
    "        # Features extras (Contexto)\n",
    "        self.user_features = torch.FloatTensor(self.data[['avg_spend', 'purchase_count']].values)\n",
    "        self.item_features = torch.FloatTensor(self.data[['popularity_score', 'avg_price']].values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'user_id': self.users[idx],\n",
    "            'item_id': self.items[idx],\n",
    "            'user_feats': self.user_features[idx],\n",
    "            'item_feats': self.item_features[idx]\n",
    "        }\n",
    "\n",
    "# --- 2. A Arquitetura Two-Tower ---\n",
    "class TwoTowerModel(pl.LightningModule):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # --- Torre do UsuÃ¡rio ---\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        # Rede Neural que combina ID + Features NumÃ©ricas\n",
    "        self.user_mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + 2, 64), # +2 pois temos 2 features numÃ©ricas de user\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32) # SaÃ­da final: vetor de tamanho 32\n",
    "        )\n",
    "        \n",
    "        # --- Torre do Item ---\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.item_mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, batch):\n",
    "        # Embeddings -> Concatenate -> MLP -> Normalize\n",
    "        # 1. Gerar Embeddings de ID\n",
    "        u_emb = self.user_embedding(batch['user_id'])\n",
    "        i_emb = self.item_embedding(batch['item_id'])\n",
    "        \n",
    "        # 2. Concatenar com features numÃ©ricas\n",
    "        u_input = torch.cat([u_emb, batch['user_feats']], dim=1)\n",
    "        i_input = torch.cat([i_emb, batch['item_feats']], dim=1)\n",
    "        \n",
    "        # 3. Passar pelos MLPs\n",
    "        user_vector = self.user_mlp(u_input)\n",
    "        item_vector = self.item_mlp(i_input)\n",
    "        \n",
    "        # 4. Normalizar vetores (para usar Cosine Similarity)\n",
    "        user_vector = F.normalize(user_vector, p=2, dim=1)\n",
    "        item_vector = F.normalize(item_vector, p=2, dim=1)\n",
    "        \n",
    "        return user_vector, item_vector\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_vector, item_vector = self(batch)\n",
    "        \n",
    "        # --- In-Batch Negatives Loss (O Segredo do Retrieval) ---\n",
    "        # Em vez de criar negativos manualmente, usamos os outros itens do batch como negativos.\n",
    "        # Se o batch tem tamanho 128, para cada usuÃ¡rio temos 1 positivo e 127 negativos.\n",
    "        \n",
    "        # Matriz de similaridade (Batch x Batch)\n",
    "        # U x I\n",
    "        scores = torch.matmul(user_vector, item_vector.T)\n",
    "        \n",
    "        # O objetivo Ã© que a diagonal principal (user i com item i) tenha score alto\n",
    "        labels = torch.arange(scores.size(0), device=self.device)\n",
    "        \n",
    "        loss = F.cross_entropy(scores * 10, labels) # *10 Ã© a \"temperatura\"\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07955b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Iniciando Treino Two-Tower. Users: 14761, Items: 8451\n",
      "TwoTowerModel(\n",
      "  (user_embedding): Embedding(14761, 32)\n",
      "  (user_mlp): Sequential(\n",
      "    (0): Linear(in_features=34, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      "  (item_embedding): Embedding(8451, 32)\n",
      "  (item_mlp): Sequential(\n",
      "    (0): Linear(in_features=34, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/model_metadata.json\", \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "    \n",
    "print(f\"ðŸš€ Iniciando Treino Two-Tower. Users: {meta['num_users']}, Items: {meta['num_items']}\")\n",
    "\n",
    "dataset = RecSysDataset(\"./data/training_dataset.parquet\")\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=4)\n",
    "\n",
    "model = TwoTowerModel(num_users=meta['num_users'], num_items=meta['num_items'])\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "999d98b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type       | Params | Mode  | FLOPs\n",
      "--------------------------------------------------------------\n",
      "0 | user_embedding | Embedding  | 472 K  | train | 0    \n",
      "1 | user_mlp       | Sequential | 4.3 K  | train | 0    \n",
      "2 | item_embedding | Embedding  | 270 K  | train | 0    \n",
      "3 | item_mlp       | Sequential | 4.3 K  | train | 0    \n",
      "--------------------------------------------------------------\n",
      "751 K     Trainable params\n",
      "0         Non-trainable params\n",
      "751 K     Total params\n",
      "3.006     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:317: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00, 13.21it/s, v_num=0, train_loss=6.050]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00,  9.12it/s, v_num=0, train_loss=6.050]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`weights_only` was not set, defaulting to `False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Modelo Treinado! Salvando artefatos...\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=5, accelerator=\"gpu\", devices=1)\n",
    "trainer.fit(model, dataloader)\n",
    "\n",
    "print(\"âœ… Modelo Treinado! Salvando artefatos...\")\n",
    "trainer.save_checkpoint(\"./data/two_tower_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144152eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
